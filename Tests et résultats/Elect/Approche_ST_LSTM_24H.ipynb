{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from holidays_es import Province\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['text.color'] = 'k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:root@localhost:5432/euproject_dhw_data')\n",
    "df=pd.read_sql_query('SELECT datetime_per_day, g1, g2, g3,ef1,gdc,gde,tmaxd,tmedia,tmind,h1,hmedia,r1  FROM data_per_1h JOIN data_per_24h ON data_per_1h.datetime_per_hour= data_per_24h.datetime_per_day',\n",
    "    con=engine, parse_dates=['datetime_per_day'], index_col='datetime_per_day')\n",
    "\n",
    "df[['g1', 'g2', 'g3']]= df[['g1', 'g2', 'g3']]*1.02264*40/ 3.6 /1000  #from m3 to Mwh\n",
    "\n",
    "\n",
    "df[['g1','g2','g3']]=df[['g1','g2','g3']].diff()\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM univarié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u=df[['ef1']]\n",
    "df_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(df_u.values, lags=50)\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled = scaler.fit_transform(df_u.values)\n",
    "    \n",
    "reframed= series_to_supervised(scaled, 10)\n",
    "reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values = reframed_differenced.values\n",
    "values = reframed.values\n",
    "n_train_days=  int(len(values) * 0.4)\n",
    "n_val_days= int(len(values) * 0.70)\n",
    "train = values[:n_train_days, :]\n",
    "val= values[n_train_days:n_val_days, :]\n",
    "test = values[n_val_days:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "val_X, val_y = val[:, :-1], val[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "print(test_X)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X= val_X.reshape((val_X.shape[0], 1,val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "print(test_X)\n",
    "print(train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "index_test=df['g1'][n_val_days:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "start_time=time.time()\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=32, validation_data=(val_X, val_y), verbose=0, shuffle=False)\n",
    "exec_time= time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='test_loss')\n",
    "plt.gca().set(title='Courbes d\\'apprentissage .', xlabel='Epochs', ylabel='Erreur')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "print(test_X.shape)\n",
    "yhat = model.predict(test_X)\n",
    "#Transform test to be 2D\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X=pd.DataFrame(test_X)\n",
    "# invert scaling for forecast\n",
    "test_X[0]= yhat\n",
    "inv_yhat = scaler.inverse_transform(test_X)\n",
    "inv_yhat = inv_yhat[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "test_X[0]= test_y\n",
    "inv_y = scaler.inverse_transform(test_X)\n",
    "inv_y = inv_y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MAE, MSE, RMSE, CV\n",
    "MAE= metrics.mean_absolute_error(inv_y, inv_yhat)\n",
    "MSE=metrics.mean_squared_error(inv_y, inv_yhat)\n",
    "CV= (np.sqrt(metrics.mean_squared_error(inv_y, inv_yhat))/inv_y.mean())*100\n",
    "R2= metrics.r2_score(inv_y, inv_yhat)\n",
    "\n",
    "print('Mean Absolute Error:', MAE)\n",
    "print('Mean Squared Error:', MSE)  \n",
    "print('Root Mean Squared Error:', np.sqrt(MSE))\n",
    "print('Coefficient of Variance:',CV)\n",
    "print('R2:', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(index_test[10:].index, inv_y, color='blue')\n",
    "plt.plot(index_test[10:].index, inv_yhat, color='red')\n",
    "plt.legend(('ELectricity', 'Electricity_forecast'))\n",
    "plt.gca().set(title='Consommation d\\'électrécité avec LSTM (Jour).', xlabel='Date', ylabel='Consumption (Mwh)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization + Save into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/Rayane/Desktop/saved_model_24H/g1_24h_model.h5')\n",
    "table_date= [df.index.min().date().strftime(\"%m/%d/%Y, %H:%M:%S\"), df.index.max().date().strftime(\"%m/%d/%Y, %H:%M:%S\"),]\n",
    "print(table_date)\n",
    "table_metric=[MAE, np.sqrt(MSE), CV]\n",
    "print(table_metric)\n",
    "print(type(table_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save into the database\n",
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    #Establishing the connection\n",
    "    conn = psycopg2.connect(database=\"euproject_dhw_data\", user='postgres', password='root', host='127.0.0.1', port= '5432')\n",
    "\n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    date = datetime.datetime.now()\n",
    "    date=date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    postgres_insert_query = \"\"\" INSERT INTO models (id_prediction, code, data_range, metrics, file) VALUES (%s,%s,%s,%s,%s)\"\"\"\n",
    "    record_to_insert = (0, 112,table_date, table_metric,\"C:/Users/Rayane/Desktop/saved_model_24H/g1_24_model.h5\")\n",
    "    cursor.execute(postgres_insert_query, record_to_insert)\n",
    "\n",
    "    conn.commit()\n",
    "    count = cursor.rowcount\n",
    "    print(count, \"Record inserted successfully into mobile table\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Failed to insert record into mobile table\", error)\n",
    "\n",
    "finally:\n",
    "    # closing database connection.\n",
    "    if conn:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:root@localhost:5432/euproject_dhw_data')\n",
    "df=pd.read_sql_query('SELECT datetime_per_day,ef1, g1, g2, g3,gdc,gde,tmaxd,tmedia,tmind,h1,hmedia,r1  FROM data_per_1h JOIN data_per_24h ON data_per_1h.datetime_per_hour= data_per_24h.datetime_per_day',\n",
    "    con=engine, parse_dates=['datetime_per_day'], index_col='datetime_per_day')\n",
    "\n",
    "df[['g1', 'g2', 'g3']]= df[['g1', 'g2', 'g3']]*1.02264*40/ 3.6 /1000  #from m3 to Mwh\n",
    "\n",
    "\n",
    "df[['g1','g2','g3']]=df[['g1','g2','g3']].diff()\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajout des attributs supplémentaires  : Mois, type du jour, jour férié\n",
    "\n",
    "def add_extra_attributes(df): \n",
    "    holidays= []\n",
    "    holidays.append(Province(name=\"malaga\",year=2018).holidays().get('local_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2018).holidays().get('national_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2018).holidays().get('regional_holidays'))\n",
    "\n",
    "    holidays.append(Province(name=\"malaga\",year=2019).holidays().get('local_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2019).holidays().get('national_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2019).holidays().get('regional_holidays'))\n",
    "\n",
    "    holidays.append(Province(name=\"malaga\",year=2020).holidays().get('local_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2020).holidays().get('national_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2020).holidays().get('regional_holidays'))\n",
    "\n",
    "    holidays.append(Province(name=\"malaga\",year=2021).holidays().get('local_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2021).holidays().get('national_holidays'))\n",
    "    holidays.append(Province(name=\"malaga\",year=2021).holidays().get('regional_holidays'))\n",
    "    \n",
    "    holidays_dates=[]\n",
    "    for i in range (len(holidays)):\n",
    "        for j in range (len(holidays[i])):\n",
    "            holidays_dates.append(holidays[i][j])\n",
    "    df_holidays=pd.DataFrame({'Holidays': holidays_dates})\n",
    "\n",
    "    df['holiday'] =0\n",
    "    df['weekday']=0\n",
    "    df['month']=0\n",
    "\n",
    "    for i in range (len(df.index)):\n",
    "        if (df.index[i].weekday() == 5 or df.index[i].weekday() == 6):\n",
    "            df['weekday'][i]=1\n",
    "        df['month'][i]= df.index[i].month\n",
    "            \n",
    "        for j in range (len(df_holidays)):\n",
    "            if (df.index[i] == df_holidays['Holidays'][j]):\n",
    "                df['holiday'][i]=1\n",
    "    return df      \n",
    "\n",
    "\n",
    "df_extra=add_extra_attributes(df)\n",
    "df_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_extra=df_extra[['ef1','g1','g2','g3','gdc','gde','tmaxd','tmedia','tmind','h1','hmedia','r1','holiday','weekday','month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled = scaler.fit_transform(df_extra.values)\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reframed= reframed.drop(reframed.columns[[4,5,6,7,8,9,10,11,12,13,14,16,17,18]], axis=1) # Prédire la consommation de la chaudière 01 à l'instant t à partir des données métérologiques à l'instant t +1 la consommation de la chaudiére 1 2 et 3 à l'instant t-1  \n",
    "df_reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reframed=df_reframed[['var1(t-1)','var2(t-1)','var3(t-1)','var4(t-1)','var5(t)','var6(t)','var7(t)','var8(t)','var9(t)','var10(t)','var11(t)','var12(t)','var13(t)','var14(t)','var15(t)','var1(t)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = df_reframed.values\n",
    "n_train_days=  int(len(values) * 0.4)\n",
    "n_val_days= int(len(values) * 0.7)\n",
    "train = values[:n_train_days, :]\n",
    "val= values[n_train_days:n_val_days, :]\n",
    "test = values[n_val_days:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "val_X, val_y = val[:, :-1], val[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X= val_X.reshape((val_X.shape[0], 1,val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sauvgarder l'index de test pour dessiner le graphe aprés\n",
    "index_test=df['g1'][n_val_days:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model1.add(Dense(1))\n",
    "model1.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "start_time=time.time()\n",
    "history = model1.fit(train_X, train_y, epochs=150, batch_size=120, validation_data=(val_X, val_y), verbose=0, shuffle=False)\n",
    "exec_time= time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='test_loss')\n",
    "plt.gca().set(title='Courbes d\\'apprentissage.', xlabel='Epochs', ylabel='Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model1.predict(test_X)\n",
    "#Transform test to be 2D\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X=pd.DataFrame(test_X)\n",
    "test_X\n",
    "# invert scaling for forecast\n",
    "test_X[0]= yhat\n",
    "inv_yhat = scaler.inverse_transform(test_X)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "#inv_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "test_X[0]= test_y\n",
    "inv_y = scaler.inverse_transform(test_X)\n",
    "inv_y = inv_y[:,0]\n",
    "#inv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MAE, MSE, RMSE, CV\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(inv_y, inv_yhat))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(inv_y, inv_yhat))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(inv_y, inv_yhat)))\n",
    "print('Coefficient of Variance:', (np.sqrt(metrics.mean_squared_error(inv_y, inv_yhat))/inv_y.mean())*100)\n",
    "print('R2:', metrics.r2_score(inv_y, inv_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(index_test[1:].index, inv_y, color='blue')\n",
    "plt.plot(index_test[1:].index, inv_yhat, color='red')\n",
    "plt.legend(('ELectricity', 'Electricity_forecast'))\n",
    "plt.gca().set(title='Consommation d\\'électrécité avec LSTM (Jour).', xlabel='Date', ylabel='Consumption (Mwh)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:root@localhost:5432/euproject_dhw_data')\n",
    "df=pd.read_sql_query('SELECT datetime_per_day, g1, g2, g3,gdc,gde,tmaxd,tmedia,tmind,h1,hmedia,r1  FROM data_per_1h JOIN data_per_24h ON data_per_1h.datetime_per_hour= data_per_24h.datetime_per_day',\n",
    "    con=engine, parse_dates=['datetime_per_day'], index_col='datetime_per_day')\n",
    "\n",
    "df[['g1', 'g2', 'g3']]= df[['g1', 'g2', 'g3']]*1.02264*40/ 3.6 /1000  #from m3 to Mwh\n",
    "\n",
    "\n",
    "df[['g1','g2','g3']]=df[['g1','g2','g3']].diff()\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra=add_extra_attributes(df)\n",
    "df_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra=df_extra[['g1','g2','g3','gde','gdc', 'h1' ,'tmaxd' ,'month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled = scaler.fit_transform(df_extra.values)\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reframed= reframed.drop(reframed.columns[[3,4,5,6,7]], axis=1) # Prédire la consommation de la chaudière 01 à l'instant t à partir des données métérologiques à l'instant t +1 la consommation de la chaudiére 1 2 et 3 à l'instant t-1  \n",
    "df_reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reframed=df_reframed[['var1(t-1)','var2(t-1)','var3(t-1)','var4(t)','var5(t)','var6(t)','var7(t)','var8(t)', 'var1(t)']]\n",
    "#df_reframed=[['var1(t-1)','var2(t-1)','var3(t-1)','var4(t)','var5(t)','var6(t)','var7(t)','var8(t)','var2(t)']]\n",
    "#df_reframed=[['var1(t-1)','var2(t-1)','var3(t-1)','var4(t)','var5(t)','var6(t)','var7(t)','var8(t)','var3(t)']]\n",
    "df_reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = df_reframed.values\n",
    "n_train_days=  int(len(values) * 0.5)\n",
    "n_val_days= int(len(values) * 0.75)\n",
    "train = values[:n_train_days, :]\n",
    "val= values[n_train_days:n_val_days, :]\n",
    "test = values[n_val_days:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "val_X, val_y = val[:, :-1], val[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X= val_X.reshape((val_X.shape[0], 1,val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model1.add(Dense(1))\n",
    "model1.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "start_time=time.time()\n",
    "history = model1.fit(train_X, train_y, epochs=100, batch_size=30, validation_data=(val_X, val_y), verbose=0, shuffle=False)\n",
    "exec_time= time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='test_loss')\n",
    "plt.gca().set(title='Courbes d\\'apprentissage .', xlabel='Epochs', ylabel='Erreur')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model1.predict(test_X)\n",
    "#Transform test to be 2D\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X=pd.DataFrame(test_X)\n",
    "test_X\n",
    "# invert scaling for forecast\n",
    "test_X[0]= yhat\n",
    "inv_yhat = scaler.inverse_transform(test_X)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "#inv_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "test_X[0]= test_y\n",
    "inv_y = scaler.inverse_transform(test_X)\n",
    "inv_y = inv_y[:,0]\n",
    "#inv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MAE, MSE, RMSE, CV\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(inv_y, inv_yhat))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(inv_y, inv_yhat))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(inv_y, inv_yhat)))\n",
    "print('Coefficient of Variance:', (np.sqrt(metrics.mean_squared_error(inv_y, inv_yhat))/inv_y.mean())*100)\n",
    "print('R2:', metrics.r2_score(inv_y, inv_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(index_test[1:].index, inv_y, color='blue')\n",
    "plt.plot(index_test[1:].index, inv_yhat, color='red')\n",
    "plt.legend(('Boiler1Consumption', 'Boiler1Consumption_forcast'))\n",
    "plt.gca().set(title='Consommation de gaz de la chaudiére N°01 (Jour).', xlabel='Date', ylabel='Consumption (Mwh)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b998da865d9ba0601a378d306c73dd5e3dc5b65106645b358f551bbb63d2a053"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
